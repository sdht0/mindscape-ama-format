###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> OllamaClient {\n  provider \"openai-generic\"\n  options {\n    model \"qwen3:8b\"\n    base_url \"http://192.168.2.95:11434/v1\"\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}\n",
    "extract_questions.baml": " function ExtractQuestions(file_text: string) -> QuestionExtraction {\n  client OllamaClient\n  prompt #\"\n    The following text lists single questions and question groups from a podcast AMA transcript.\n    Single question consist of just a name and a question.\n    Question group consist of multiple names and questions, all surrounded by asterisks.\n\n    Your tasks:\n    1. Count how many single questions + question groups there are.\n       All questions within asterisks are considered to be part of the SAME question.\n    2. For each question/group, produce a ONE-LINE summary consisting of:\n       - The name(s) mentioned in the question.\n       - A short topic description (6-12 words) for quick reference.\n    The number of lines in step 2 should EXACTLY match the count from step 1.\n    Example line: \"Alice & Bob: career trajectory and early challenges\"\n\n    Output strictly as JSON according to the output schema below and NOTHING else.\n\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }}\n    --- BEGIN QUESTIONS FILE ---\n    {{ file_text }}\n    --- END QUESTIONS FILE ---\n  \"#\n}\n\nclass QuestionExtraction {\n  count int @description(\"Total number of questions or question groups found\")\n  lines string[] @description(\"Array of one-line summaries, one per question/group, length == count\")\n}\n",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../src/mindscape-ama-format\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.90.2\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "locate_snippets.baml": "function LocateSnippets(question: string, chunk: string) -> Snippets {\n  client \"openai/gpt-4.1-mini\"\n  prompt #\"\n    You will be provided with:\n    1. A podcast TRANSCRIPT CHUNK.\n    2. Either (i) a single name and a question snipper OR (ii) multiple names and corresponding question snippet, either of which appears somewhere inside the TRANSCRIPT CHUNK.\n\n    Your task:\n    • Find the exact location where that question begins inside the chunk.\n    • Find the exact location where the question ends and the answer begins.\n\n    Return (i) Whether the question was found, (ii) The first 10 words of the question exactly as they appear in the TRANSCRIPT CHUNK, *including* the asker name and any prefix text such as \"let me group these questions\", (iii) Whether the answer was found, and (iv) the first 10 words of the answer exactly as they appear in the chunk.\n\n    Verify that the 10 words snippet MUST be a substring in TRANSCRIPT CHUNK without any breaks.\n\n    ---- Example ----\n    QUESTION:\n    ---\n    Eric: I’m missing something about “information”. If one book ... alien language carrying even more insight than the text book?\n    ---\n    TRANSCRIPT CHUNK:\n    ---\n    to live either. That's something that you want to get bigger, but you also want to have a pleasurable existence while you are alive. And maybe that involves eating some pizzas. So you have to balance all these things the best you can. Eric says, \"I'm missing something about information. If one book was written in gibberish and another with equal amounts of ink and paper, was a quantum mechanics textbook, and both were thrown into a fire, it would take the same amount of information to reconstruct them, if the gibberish were constructed correctly. How can we quantify the additional information in the quantum mechanics book versus gibberish? What if, unbeknownst to us, the gibberish were actually an alien language carrying even more insight than the textbook?\" Yeah, these are perfectly good questions. I talk about this a little bit in the big picture, in fact. There's a famous example of the Voynich manuscript,\n    ---\n    OUTPUT (example not in json):\n    question_found = true\n    question_snippet = Eric says, \"I'm missing something about information. If one book\n    answer_found = true\n    answer_snippet = Yeah, these are perfectly good questions. I talk about this\n    ---- End Example ----\n\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }}\n    QUESTION:\n    ---\n    {{ question }}\n    ---\n\n    TRANSCRIPT CHUNK:\n    ---\n    {{ chunk }}\n    ---\n\n    Respond strictly with a JSON object that matches the schema above and no other text.\n  \"#\n}\n\nclass Snippets {\n  question_found bool @description(\"Whether the start of the question was found or not\")\n  question_snippet string @description(\"First 10 words of the question, verbatim from chunk\")\n  answer_found bool @description(\"Whether the start of the answer was found or not\")\n  answer_snippet string @description(\"First 10 words of the answer, verbatim from chunk\")\n}\n",
}

def get_baml_files():
    return file_map
